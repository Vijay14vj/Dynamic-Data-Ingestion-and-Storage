# Dynamic Data Ingestion and Storage in HDFS with Automated Hive Integration

## ğŸ“Œ Project Overview

This project focuses on dynamically ingesting data from a remote source, storing it in HDFS (Hadoop Distributed File System), and integrating it with Hive for structured querying and analysis. The entire process is automated using Python and Shell scripting to ensure efficient and repeatable data processing.

---

## ğŸš€ Objective

- Fetch data from a public dataset link
- Store it in HDFS using Hadoop CLI
- Create and load data into a Hive table for visualization and query
- Automate the process for future ingestion and refreshes

---

## ğŸ› ï¸ Technologies Used

- **Python** â€“ For scripting and automation
- **Shell Scripting** â€“ For handling Hadoop and Hive CLI operations
- **HDFS** â€“ For distributed data storage
- **Apache Hive** â€“ For data warehousing and querying
- **wget / curl** â€“ For downloading files from the internet
- **Cloudera VM (CentOS)** â€“ Virtual Hadoop environment for development and testing

---

## ğŸ§© Problem Statement

Automate the ingestion of census population data from a public source, store it on HDFS, and integrate it with Hive for structured access and visualization. Ensure modular and maintainable code, adhering to Python standards (PEP8).


##Author

Vijay M
